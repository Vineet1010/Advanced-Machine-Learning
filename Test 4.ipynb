{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "\n",
    "df= pd.read_csv(\"EEG_data_confusion.csv\")\n",
    "mean_Att = round(df[\"Attention\"].mean(), 3)\n",
    "mean_Med = round(df[\"Mediation\"].mean(), 3)\n",
    "\n",
    "new = pd.DataFrame(columns = {\"('Attention', \" + str(mean_Att) + ')', \"('Mediation', \" + str(mean_Med) + ')'})\n",
    "new.to_csv(\"output.csv\", index = False)\n",
    "\n",
    "df_scaled = StandardScaler().fit_transform(df)\n",
    "pca = decomposition.PCA(n_components = 5)\n",
    "Transformed_df = pca.fit_transform(df_scaled)\n",
    "var = pca.explained_variance_ratio_\n",
    "var = round(var.sum()*100, 2)\n",
    "out = pd.DataFrame(columns = {\"Variance Explained by 5 PCA Components is \" + str(var) + '%'})\n",
    "out.to_csv(\"output1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from apyori import apriori\n",
    "\n",
    "def uRecords():\n",
    "    df = pd.read_csv(\"Symptoms.csv\")\n",
    "    df[\"Records\"] = '[' + df['Symptoms'].astype(str) + ']'\n",
    "    rec = pd.Series(df['Records'])\n",
    "    return rec\n",
    "\n",
    "def nUsers(Symptom):\n",
    "    df = pd.read_csv(\"Symptoms.csv\")\n",
    "    count = df['Symptoms'].str.count(Symptom).sum()\n",
    "    return count\n",
    "\n",
    "def commonSymptom(Symptom):\n",
    "    association_rules = apriori(transactions = uRecords(), min_support=0.0002, min_confidence=0.002, min_lift=1.1, min_length=2, max_length=2)\n",
    "    association_results = list(association_rules)\n",
    "    print(association_results)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"CricketMatchDataset.csv\")\n",
    "mean = df.agg({\"Team 1 20ovr Score\" : np.mean}).values\n",
    "mean = round(*mean, 3)\n",
    "\n",
    "count = df[\"Final Result\"].str.contains(r'Home Team').sum()\n",
    "\n",
    "df1 = df[['Home Team', 'Team 1 Batsman 1 Score']]\n",
    "df2 = df1.groupby('Home Team', as_index = False).mean()\n",
    "df_i = df2[df2['Home Team'] == 'India']\n",
    "avg_i = round(df_i[\"Team 1 Batsman 1 Score\"].iloc[0], 3)\n",
    "\n",
    "df1 = df[((df['Home Team'] == 'India') & (df['Team Batting First'] == 'Home Team'))]\n",
    "df2 = df[((df['Away Team'] == 'India') & (df['Team Batting First'] == 'Away Team'))]\n",
    "df3 = df1.append(df2)\n",
    "mean_i = round(df3[\"Team 1 Batsman 1 Score\"].mean(axis = 0), 3)\n",
    "\n",
    "df.drop(['Date of Match'], axis = 1, inplace = True)\n",
    "X = df.drop('Final Result', axis = 1)\n",
    "X = pd.get_dummies(X, prefix_sep = '_', drop_first = False)\n",
    "no_of_col = len(X.columns)\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(df['Final Result'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "lr_cric = LogisticRegression(C = 1.0, solver = 'liblinear', multi_class = 'ovr')\n",
    "lr_cric.fit(X_train, y_train)\n",
    "y_pred = lr_cric.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, y_pred), 3)\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(mean))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(avg_i))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(mean_i))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(no_of_col))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "df = pd.read_csv(\"songs.csv\")\n",
    "\n",
    "#df['text'] = df['text'].apply(lambda x: re.sub('[!\"#$%&\\'()*+,-./:;<->?@[\\\\]^_`{|}~]'))\n",
    "my_stop = set(stopwords.words('english') + [x for x in punctuation])\n",
    "vectorizer = CountVectorizer(min_df = 20, max_df = 500, lowercase = True, stop_words = my_stop)\n",
    "\n",
    "new_vector = vectorizer.fit_transform(df['text'])\n",
    "rows, columns = new_vector.shape\n",
    "\n",
    "clusters = KMeans(n_clusters = 4, random_state = 2)\n",
    "clusters.fit(new_vector)\n",
    "\n",
    "df['labels'] = clusters.labels_\n",
    "dfk = df.groupby('labels').describe().transpose()\n",
    "no_large = dfk.iloc[4][1]\n",
    "no_NY = dfk.iloc[4][2]\n",
    "no_artist = dfk.iloc[2][1]\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(columns))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(no_large))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(no_NY))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(no_artist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
